# -*- coding: utf-8 -*-
"""breast_cancer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W26zfm-ZP4uVnm5t8k8hfMYc1rMtOqv5
"""

#Description: This program detects breast cancer, based off of data.

# Loading Libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load Dataset
from google.colab import files
uploaded = files.upload()
df = pd.read_csv('data.csv')
df.head(7)

# Count the number of rows and columns
df.shape

# count Nan
df.isna().sum()

# drop the column
df=df.dropna(axis=1)

df.shape

# get a count of number of malignant or benign cells
df['diagnosis'].value_counts()

# visulaize the count
sns.countplot(df['diagnosis'], label='count')

# look at the data types of columns
df.dtypes

# Encode the categorical data
from sklearn.preprocessing import LabelEncoder
labelencoder_Y = LabelEncoder()
df.iloc[:,1] = labelencoder_Y.fit_transform(df.iloc[:,1].values)

# Create a pair plot
sns.pairplot(df.iloc[:,1:5], hue='diagnosis')

# print the first 5 rows of the new data
df.head(5)

#get the correlation of the columns
df.iloc[:,1:12].corr()

#visualize the correlation
plt.figure(figsize=(10,10))
sns.heatmap(df.iloc[:,1:12].corr(), annot=True, fmt='.0%')

plt.figure(figsize=(20,20))  
sns.heatmap(df.corr(), annot=True, fmt='.0%')

# Split the dataset into independent (X) dependent (Y) data sets
X = df.iloc[:,2:31].values
Y = df.iloc[:,1].values

# Splitting the dataset into 75% training and 25% testing
from sklearn.model_selection import train_test_split
X_train, X_test,Y_train,Y_test = train_test_split(X, Y, test_size=0.25, random_state=0)

# Scale the data (Feature Scaling)
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.fit_transform(X_test)

# Create a function for the models
import time
def models(X_train, Y_train):

  # Logistic Regression
  t0 = time.time()
  from sklearn.linear_model import LogisticRegression
  log = LogisticRegression(random_state=0)
  log.fit(X_train, Y_train)
  t1 = time.time()
  print("Time taken by Logistic Regression - {:.2} s".format(t1 - t0))

  # Using KNeighborsClassifier
  t0 = time.time() 
  from sklearn.neighbors import KNeighborsClassifier
  knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
  knn.fit(X_train, Y_train)
  t1 = time.time()
  print("Time taken by KneighboursClassifier - {:.2} s".format(t1 - t0))

  # Using SVC linear
  t0 = time.time()
  from sklearn.svm import SVC
  svc_lin = SVC(kernel = 'linear', random_state = 0)
  svc_lin.fit(X_train, Y_train)
  t1 = time.time()
  print("Time taken by SVC - {:.2} s".format(t1 - t0))
  '''
  #Using SVC rbf
  from sklearn.svm import SVC
  svc_rbf = SVC(kernel = 'rbf', random_state = 0)
  svc_rbf.fit(X_train, Y_train)

  #Using GaussianNB 
  from sklearn.naive_bayes import GaussianNB
  gauss = GaussianNB()
  gauss.fit(X_train, Y_train)

  #Using DecisionTreeClassifier 
  from sklearn.tree import DecisionTreeClassifier
  tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
  tree.fit(X_train, Y_train)

  #Using RandomForestClassifier method of ensemble class to use Random Forest Classification algorithm
  from sklearn.ensemble import RandomForestClassifier
  forest = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
  forest.fit(X_train, Y_train)

  '''
  #print model accuracy on the training data.
  print('[0]Logistic Regression Training Accuracy:', log.score(X_train, Y_train))
  print('[1]K Nearest Neighbor Training Accuracy:', knn.score(X_train, Y_train))
  print('[2]Support Vector Machine (Linear Classifier) Training Accuracy:', svc_lin.score(X_train, Y_train))
  '''
  print('[3]Support Vector Machine (RBF Classifier) Training Accuracy:', svc_rbf.score(X_train, Y_train))
  print('[4]Gaussian Naive Bayes Training Accuracy:', gauss.score(X_train, Y_train))
  print('[5]Decision Tree Classifier Training Accuracy:', tree.score(X_train, Y_train))
  print('[6]Random Forest Classifier Training Accuracy:', forest.score(X_train, Y_train))'''
  return log, knn, svc_lin

#testing the data
# True positive (TP) = the number of cases correctly identified as patient. 
# False positive (FP) = the number of cases incorrectly identified as patient.
# True negative (TN) = the number of cases correctly identified as healthy.
# False negative (FN) = the number of cases incorrectly identified as healthy.
model = models(X_train, Y_train)
#howsvm is better than other classifiers
from sklearn.metrics import confusion_matrix                                 
for i in range(len(model)):
  cm = confusion_matrix(Y_test, model[i].predict(X_test))
  
  TN = cm[0][0]
  TP = cm[1][1]
  FN = cm[1][0]
  FP = cm[0][1]
  
  print(cm)
  print('Model[{}] Testing Accuracy = "{}!"'.format(i,  (TP + TN) / (TP + TN + FN + FP)))
  print()# Print a new line

#Show other ways to get the classification accuracy & other metrics 

from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

for i in range(len(model)):
  print('Model ',i)
  #Check precision, recall, f1-score
  print( classification_report(Y_test, model[i].predict(X_test)) )
  #Another way to get the models accuracy on the test data
  print( accuracy_score(Y_test, model[i].predict(X_test)))
  print()#Print a new line

#combining classifiers using Voting\Stacking Classification

from sklearn.linear_model import LogisticRegression
log = LogisticRegression(random_state=0)

# Using KNeighborsClassifier
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)

# Using SVC linear
from sklearn.svm import SVC
svc_lin = SVC(kernel = 'linear', random_state = 0)

from sklearn.ensemble import VotingClassifier
from sklearn.metrics import accuracy_score, f1_score, log_loss
voting_clf = VotingClassifier(estimators=[ ('LogReg', log), ('Knn', knn)], voting='hard')       #soft can be done there (97%)
                                                                                                #Hard voting -> models that predict class labels
                                                                                                #Soft voting -> models that predict class membership probabilities
voting_clf.fit(X_train, Y_train)
preds = voting_clf.predict(X_test)
acc = accuracy_score(Y_test, preds)
l_loss = log_loss(Y_test, preds)
f1 = f1_score(Y_test, preds)

print("Testing Accuracy is: " + str(acc))

#combining classifiers using Voting\Stacking Classification

from sklearn.linear_model import LogisticRegression
log = LogisticRegression(random_state=0)


# Using SVC linear
from sklearn.svm import SVC
svc_lin = SVC(kernel = 'linear', random_state = 0)

from sklearn.ensemble import VotingClassifier
from sklearn.metrics import accuracy_score, f1_score, log_loss
voting_clf = VotingClassifier(estimators=[ ('SvcLin', svc_lin),('LogReg', log) ], voting='hard')
voting_clf.fit(X_train, Y_train)
preds = voting_clf.predict(X_test)
acc = accuracy_score(Y_test, preds)
l_loss = log_loss(Y_test, preds)
f1 = f1_score(Y_test, preds)

print("Testing Accuracy is: " + str(acc))

#combining classifiers using Voting\Stacking Classification

from sklearn.linear_model import LogisticRegression
log = LogisticRegression(random_state=0)

# Using KNeighborsClassifier
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)

# Using SVC linear
from sklearn.svm import SVC
svc_lin = SVC(kernel = 'linear', random_state = 0)

from sklearn.ensemble import VotingClassifier
from sklearn.metrics import accuracy_score, f1_score, log_loss
voting_clf = VotingClassifier(estimators=[('SvcLin', svc_lin),('Knn', knn) ], voting='hard')
voting_clf.fit(X_train, Y_train)
preds = voting_clf.predict(X_test)
acc = accuracy_score(Y_test, preds)
l_loss = log_loss(Y_test, preds)
f1 = f1_score(Y_test, preds)

print("Testing Accuracy is: " + str(acc))

#combining classifiers using Voting\Stacking Classification

from sklearn.linear_model import LogisticRegression
log = LogisticRegression(random_state=0)

# Using KNeighborsClassifier
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)

# Using SVC linear
from sklearn.svm import SVC
svc_lin = SVC(kernel = 'linear', random_state = 0)

from sklearn.ensemble import VotingClassifier
from sklearn.metrics import accuracy_score, f1_score, log_loss
voting_clf = VotingClassifier(estimators=[ ('LogReg', log), ('Knn', knn), ('SvcLin', svc_lin)], voting='hard')
voting_clf.fit(X_train, Y_train)
preds = voting_clf.predict(X_test)
acc = accuracy_score(Y_test, preds)
l_loss = log_loss(Y_test, preds)
f1 = f1_score(Y_test, preds)

print("Testing Accuracy is: " + str(acc))

# from sklearn.ensemble import RandomForestClassifier 
# from sklearn.ensemble import VotingClassifier
# from sklearn.linear_model import LogisticRegression
# from sklearn.svm import SVC

# log_clf = LogisticRegression()
# rnd_clf = RandomForestClassifier()
# svm_clf = SVC()
# voting_clf = VotingClassifier( estimators=[('lr', log_clf), ('r', rnd_clf), ('svc', svm_clf)], voting='hard' )
# voting_clf.fit(X_train, Y_train)

# from sklearn.metrics import accuracy_score
# for clf in (log_clf, rnd_clf, svm_clf, voting_clf):
#     clf.fit(X_train, Y_train)
#     y_pred = clf.predict(X_test)
#     print(clf.__class__.__name__, accuracy_score(Y_test, y_pred))

# # By tarun by combining classifiers using Voting\Stacking Classification

# from sklearn.linear_model import LogisticRegression
# log = LogisticRegression(random_state=0)

# # Using KNeighborsClassifier
# from sklearn.neighbors import KNeighborsClassifier
# knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)

# # Using SVC linear
# from sklearn.svm import SVC
# svc_lin = SVC(kernel = 'linear', random_state = 0)

# #Using SVC rbf
# from sklearn.svm import SVC
# svc_rbf = SVC(kernel = 'rbf', random_state = 0)

# #Using GaussianNB 
# from sklearn.naive_bayes import GaussianNB
# gauss = GaussianNB()

# #Using DecisionTreeClassifier 
# from sklearn.tree import DecisionTreeClassifier
# tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)

# #Using RandomForestClassifier method of ensemble class to use Random Forest Classification algorithm
# from sklearn.ensemble import RandomForestClassifier
# forest = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)

# from sklearn.ensemble import VotingClassifier
# from sklearn.metrics import accuracy_score, f1_score, log_loss
# voting_clf = VotingClassifier(estimators=[('SvcLin', svc_lin),('SvcRbf', svc_rbf),('Knn', knn),('Gauss', gauss),('forest', forest), ('DTree', tree), ('LogReg', log)], voting='hard')
# voting_clf.fit(X_train, Y_train)
# preds = voting_clf.predict(X_test)
# acc = accuracy_score(Y_test, preds)
# l_loss = log_loss(Y_test, preds)
# f1 = f1_score(Y_test, preds)

# print("Accuracy is: " + str(acc))
# print("Log Loss is: " + str(l_loss))
# print("F1 Score is: " + str(f1))

# # Combination of Classifiers
# def models_combine(X_train, Y_train, X_test, Y_test):
  
#   from sklearn.linear_model import LogisticRegression
#   log = LogisticRegression(random_state=0)
#   log.fit(X_train, Y_train)
#   X_test = log.fit(X_train, Y_train) # t1 = time.time()
#   Y_test = log.fit(X_train, Y_train) # t1 = time.time()
#   # print(log)
#   from sklearn.svm import SVC
#   svc_lin = SVC(kernel = 'linear', random_state = 0)
#   svc_lin.fit(X_test, Y_test)
#   return svc_lin

#getting all models
model = models(X_train, Y_train)

